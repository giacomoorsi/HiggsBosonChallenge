{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *\n",
    "from functions import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, tX_test, ids = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test only for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_PRI = 22\n",
    "\n",
    "y_0 =  y[tX[:, i_PRI]==0]\n",
    "tx_0 = tX[tX[:, i_PRI]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_0_filtered = np.delete(tx_0, [4,5,6,12,22,23,24,25,26,27,28,29], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_0_filtered[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean=None, std=None):\n",
    "    \"\"\"Standardize data set.\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(x, axis=0)\n",
    "    x = x - mean\n",
    "\n",
    "    if std is None:\n",
    "        std = np.nanstd(x, axis=0)\n",
    "    x = x / std\n",
    "    return x, mean, std\n",
    "\n",
    "\n",
    "def get_jet_index(x):\n",
    "    \"\"\"Get index of three groups.\"\"\"\n",
    "    jet0_index = np.where(x[:,22]==0)[0]\n",
    "    jet1_index = np.where(x[:,22]==1)[0]\n",
    "    jet2_index = np.where(x[:,22]>=2)[0]\n",
    "    return [jet0_index, jet1_index, jet2_index]\n",
    "\n",
    "def delta_angle_norm(a, b):\n",
    "    \"\"\"Caluculate difference between two angles\n",
    "    normalize the result to ]-pi, pi].\"\"\"\n",
    "    delta = a - b\n",
    "    delta[delta < -np.pi] += 2 * np.pi\n",
    "    delta[delta >  np.pi] -= 2 * np.pi\n",
    "    return delta\n",
    "\n",
    "\n",
    "def add_phi(x):\n",
    "    \"\"\"Add new phi features.\"\"\"\n",
    "    # PRI_lep_phi - PRI_tau_phi\n",
    "    r1 = delta_angle_norm(x[:,18], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_met_phi - PRI_tau_phi\n",
    "    r2 = delta_angle_norm(x[:,20], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_jet_leading_phi - PRI_tau_phi\n",
    "    r3 = delta_angle_norm(x[:,25], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_jet_subleading_phi - PRI_tau_phi\n",
    "    r4 = delta_angle_norm(x[:,28], x[:,15]).reshape(-1, 1)\n",
    "\n",
    "    x = np.concatenate([x, r1, r2, r3, r4], axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def apply_log1p(x):\n",
    "    \"\"\"Apply log normalization to features with long tail.\"\"\"\n",
    "    long_tail = [0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "    x[:, long_tail] = np.log1p(x[:, long_tail])\n",
    "    return x\n",
    "\n",
    "\n",
    "def drop_useless(x):\n",
    "    \"\"\"Drop useless columns.\"\"\"\n",
    "    # raw angles\n",
    "    # eta: 14, 17, 24, 27\n",
    "    # phi: 15, 18, 20, 25, 28\n",
    "    raw_angle = [15, 18, 20, 25, 28]\n",
    "    # columns of the same value (std is 0)\n",
    "    same_cols = list(np.where(np.nanstd(x, axis=0)==0)[0])\n",
    "    # columns full of NaN\n",
    "    nan_cols = list(np.where(np.all(np.isnan(x), axis=0))[0])\n",
    "\n",
    "    to_drop = list(set(raw_angle+same_cols+nan_cols))\n",
    "    x = np.delete(x, to_drop, axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fill_missing(x):\n",
    "    \"\"\"Fill missing values.\"\"\"\n",
    "    # use nan as missing value\n",
    "    x[x==-999] = np.nan\n",
    "    return x\n",
    "\n",
    "\n",
    "def fill_nan(x):\n",
    "    \"\"\"Fill nan values.\"\"\"\n",
    "    # fill nan with 0\n",
    "    x = np.nan_to_num(x)\n",
    "\n",
    "    # # fill nan with the most frequently elements\n",
    "    # for i in range(x.shape[1]):\n",
    "    #     xi = x[:, i]\n",
    "    #     value, count = np.unique(xi, return_counts=True)\n",
    "    #     mode = value[np.argmax(count)]\n",
    "    #     xi[np.isnan(xi)] = mode\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocessing(x_train, x_test):\n",
    "    \"\"\"Preprocess data.\"\"\"\n",
    "    # fill missing values with nan\n",
    "    x_train = fill_missing(x_train)\n",
    "    x_test = fill_missing(x_test)\n",
    "\n",
    "    # add new phi features\n",
    "    x_train = add_phi(x_train)\n",
    "    x_test = add_phi(x_test)\n",
    "\n",
    "    # apply log normalization\n",
    "    x_train = apply_log1p(x_train)\n",
    "    x_test = apply_log1p(x_test)\n",
    "\n",
    "    # drop useless columns\n",
    "    x_train = drop_useless(x_train)\n",
    "    x_test = drop_useless(x_test)\n",
    "\n",
    "    # standardization\n",
    "    x_train, mean, std = standardize(x_train)\n",
    "    x_test, _, _ = standardize(x_test, mean, std)\n",
    "\n",
    "    # fill nan\n",
    "    x_train = fill_nan(x_train)\n",
    "    x_test = fill_nan(x_test)\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_p, aaa = preprocessing(tX, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tX_p, tX_test = preprocessing(tX, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_expansion(tx_0_filtered, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    test_indices = np.zeros(len(y)).astype(bool)\n",
    "    test_indices[k_indices[k]] = True\n",
    "    train_indices = (~test_indices).tolist()\n",
    "    test_indices = test_indices.tolist()\n",
    "        \n",
    "    x_train = x[train_indices, :]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    x_test = x[test_indices, :]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    \n",
    "    x_train_expanded = feature_expansion(x_train, degree)\n",
    "    x_test_expanded = feature_expansion(x_test, degree)\n",
    "\n",
    "    print(\"x_train_expanded\", x_train_expanded)\n",
    "\n",
    "    \n",
    "    w = ridge_regression(y_train, x_train_expanded, lambda_)\n",
    "    \n",
    "    \n",
    "    loss_tr = math.sqrt(2*compute_loss(y_train, x_train_expanded, w))\n",
    "    \n",
    "    loss_te = math.sqrt(2*compute_loss(y_test, x_test_expanded, w))\n",
    "    \n",
    "    classified = sum(predict_labels(w, x_test_expanded)==y_test)/len(y_test)\n",
    "    \n",
    "\n",
    "    return loss_tr, loss_te, classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_advanced_demo(x, y):\n",
    "    print(\"Dimensione x: \", x.shape)\n",
    "\n",
    "    seed = 1\n",
    "    degrees = [3]#np.arange(3,10)\n",
    "    k_fold = 2\n",
    "    #lambdas = np.logspace(-15, -8, 10)\n",
    "    lambdas = [0]\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    \n",
    "\n",
    "    tr_total = []\n",
    "    te_total = []\n",
    "\n",
    "    min_te = float('inf')\n",
    "    min_degree = 0\n",
    "    min_lambda = 0\n",
    "    \n",
    "    for degree in degrees : \n",
    "        \n",
    "      #  print(\"vado con grado\", degree)\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        rmse_classification = []\n",
    "        rmse_tr_var = []\n",
    "        rmse_te_var = []\n",
    "    \n",
    "        for lambda_ in lambdas : \n",
    "        #    print(\"vado con lambda\", lambda_)\n",
    "\n",
    "\n",
    "            loss_tr = []\n",
    "            loss_te = []\n",
    "            loss_classification = []\n",
    "\n",
    "            for k in range(k_fold) : \n",
    "                tr, te, classified =  cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                loss_tr.append(tr)\n",
    "                loss_te.append(te)\n",
    "                loss_classification.append(classified)\n",
    "\n",
    "            rmse_tr.append(np.mean(loss_tr))\n",
    "            rmse_classification.append(np.mean(loss_classification))\n",
    "\n",
    "\n",
    "            rmse_te.append(np.mean(loss_te))\n",
    "            rmse_tr_var.append(np.var(loss_tr))\n",
    "            rmse_te_var.append(np.var(loss_te))\n",
    "            \n",
    "            print(\"Grado\", degree, \", lambda\", lambda_, \", % giusti: \", np.mean(loss_classification))\n",
    "            \n",
    "            if np.mean(loss_te) < min_te : \n",
    "                min_te = np.mean(loss_te)\n",
    "                min_degree = degree\n",
    "                min_lambda = lambda_\n",
    "    \n",
    "        tr_total.append(rmse_tr)\n",
    "        te_total.append(rmse_te)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    cross_validation_advanced_visualization(lambdas, degrees, tr_total, te_total)\n",
    "    \n",
    "    print(\"min_degree\", min_degree)\n",
    "    print(\"min_lambda\", min_lambda)\n",
    "\n",
    "    \n",
    "    #cross_validation_advanced_visualization(lambdas, degrees, tr_total, te_total)\n",
    "    return lambdas, degrees, tr_total, te_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_advanced_demo(tX_p, y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, degrees, tr_total, te_total = cross_validation_advanced_demo(tX_p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_p = np.reshape(y, (len(y), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_PRI = 22\n",
    "y_jet0  = y[tX[:, i_PRI]==0]\n",
    "tx_jet0 = tX[tX[:, i_PRI]==0]\n",
    "\n",
    "y_jet1  = y[ tX[:, i_PRI] == 1]\n",
    "tx_jet1 = tX[tX[:, i_PRI] == 1]\n",
    "\n",
    "y_jet2  = y[ tX[:, i_PRI] > 1]\n",
    "tx_jet2 = tX[tX[:, i_PRI] > 1]\n",
    "#----------------------------------\n",
    "#Then it can be executed like this\n",
    "tx_0_filtered = np.delete(tx_jet0, [4,5,6,12,22,23,24,25,26,27,28], axis=1)\n",
    "tx_1_filtered = np.delete(tx_jet1, [4,5,6,12,22,26,27,28], axis=1)\n",
    "tx_2_filtered = np.delete(tx_jet2, [22], axis=1)\n",
    "\n",
    "tx_0_filtered = fill_nan(fill_missing(tx_0_filtered))\n",
    "tx_1_filtered = fill_nan(fill_missing(tx_1_filtered))\n",
    "tx_2_filtered = fill_nan(fill_missing(tx_2_filtered))\n",
    "\n",
    "tx_train_0 = featureExpand(tx_0_filtered, 0)\n",
    "tx_train_1 = featureExpand(tx_1_filtered, 1)\n",
    "tx_train_2 = featureExpand(tx_2_filtered, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 500\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        classified = sum(predict_labels(w, tx)==y)/len(y)\n",
    "        # log info\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, classified={c}\".format(i=iter, l=loss, c=classified))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    return w\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\",True)\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jet0r = y_jet0.reshape((len(y_jet0), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = logistic_regression_penalized_gradient_descent_demo(y_jet0r, tx_0_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = np.c_[np.ones((y.shape[0], 1)), tX_p]\n",
    "classified = sum(predict_labels(w, tx)==y_p)/len(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = sum(predict_labels(w, tx_test)==y_p)/len(y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "047d5c4a70aa4e5ae964d8b25b83a0a6056fc4ba4dd2c3a708bdfa354f913a43"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}