{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *\n",
    "from functions import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y_test, tX_test, ids = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test only for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "cell_type": "code",
   "execution_count": 5,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "i_PRI = 22\n",
    "\n",
    "y_0 =  y[tX[:, i_PRI]==0]\n",
    "tx_0 = tX[tX[:, i_PRI]==0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_0_filtered = np.delete(tx_0, [4,5,6,12,22,23,24,25,26,27,28,29], axis=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([175.864,  16.915, 134.805,  16.405,   3.891,  16.405,  57.983,\n",
       "         1.056,  -1.385,  28.209,  -2.197,  -2.231,  29.774,   0.798,\n",
       "         1.569,   2.723,  -0.871,  53.131])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "source": [
    "tx_0_filtered[1,:]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 8,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x, mean=None, std=None):\n",
    "    \"\"\"Standardize data set.\"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(x, axis=0)\n",
    "    x = x - mean\n",
    "\n",
    "    if std is None:\n",
    "        std = np.nanstd(x, axis=0)\n",
    "    x = x / std\n",
    "    return x, mean, std\n",
    "\n",
    "\n",
    "def get_jet_index(x):\n",
    "    \"\"\"Get index of three groups.\"\"\"\n",
    "    jet0_index = np.where(x[:,22]==0)[0]\n",
    "    jet1_index = np.where(x[:,22]==1)[0]\n",
    "    jet2_index = np.where(x[:,22]>=2)[0]\n",
    "    return [jet0_index, jet1_index, jet2_index]\n",
    "\n",
    "def delta_angle_norm(a, b):\n",
    "    \"\"\"Caluculate difference between two angles\n",
    "    normalize the result to ]-pi, pi].\"\"\"\n",
    "    delta = a - b\n",
    "    delta[delta < -np.pi] += 2 * np.pi\n",
    "    delta[delta >  np.pi] -= 2 * np.pi\n",
    "    return delta\n",
    "\n",
    "\n",
    "def add_phi(x):\n",
    "    \"\"\"Add new phi features.\"\"\"\n",
    "    # PRI_lep_phi - PRI_tau_phi\n",
    "    r1 = delta_angle_norm(x[:,18], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_met_phi - PRI_tau_phi\n",
    "    r2 = delta_angle_norm(x[:,20], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_jet_leading_phi - PRI_tau_phi\n",
    "    r3 = delta_angle_norm(x[:,25], x[:,15]).reshape(-1, 1)\n",
    "    # PRI_jet_subleading_phi - PRI_tau_phi\n",
    "    r4 = delta_angle_norm(x[:,28], x[:,15]).reshape(-1, 1)\n",
    "\n",
    "    x = np.concatenate([x, r1, r2, r3, r4], axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def apply_log1p(x):\n",
    "    \"\"\"Apply log normalization to features with long tail.\"\"\"\n",
    "    long_tail = [0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 19, 21, 23, 26, 29]\n",
    "    x[:, long_tail] = np.log1p(x[:, long_tail])\n",
    "    return x\n",
    "\n",
    "\n",
    "def drop_useless(x):\n",
    "    \"\"\"Drop useless columns.\"\"\"\n",
    "    # raw angles\n",
    "    # eta: 14, 17, 24, 27\n",
    "    # phi: 15, 18, 20, 25, 28\n",
    "    raw_angle = [15, 18, 20, 25, 28]\n",
    "    # columns of the same value (std is 0)\n",
    "    same_cols = list(np.where(np.nanstd(x, axis=0)==0)[0])\n",
    "    # columns full of NaN\n",
    "    nan_cols = list(np.where(np.all(np.isnan(x), axis=0))[0])\n",
    "\n",
    "    to_drop = list(set(raw_angle+same_cols+nan_cols))\n",
    "    x = np.delete(x, to_drop, axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fill_missing(x):\n",
    "    \"\"\"Fill missing values.\"\"\"\n",
    "    # use nan as missing value\n",
    "    x[x==-999] = np.nan\n",
    "    return x\n",
    "\n",
    "\n",
    "def fill_nan(x):\n",
    "    \"\"\"Fill nan values.\"\"\"\n",
    "    # fill nan with 0\n",
    "    x = np.nan_to_num(x)\n",
    "\n",
    "    # # fill nan with the most frequently elements\n",
    "    # for i in range(x.shape[1]):\n",
    "    #     xi = x[:, i]\n",
    "    #     value, count = np.unique(xi, return_counts=True)\n",
    "    #     mode = value[np.argmax(count)]\n",
    "    #     xi[np.isnan(xi)] = mode\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocessing(x_train, x_test):\n",
    "    \"\"\"Preprocess data.\"\"\"\n",
    "    # fill missing values with nan\n",
    "    x_train = fill_missing(x_train)\n",
    "    x_test = fill_missing(x_test)\n",
    "\n",
    "    # add new phi features\n",
    "    x_train = add_phi(x_train)\n",
    "    x_test = add_phi(x_test)\n",
    "\n",
    "    # apply log normalization\n",
    "    x_train = apply_log1p(x_train)\n",
    "    x_test = apply_log1p(x_test)\n",
    "\n",
    "    # drop useless columns\n",
    "    x_train = drop_useless(x_train)\n",
    "    x_test = drop_useless(x_test)\n",
    "\n",
    "    # standardization\n",
    "    x_train, mean, std = standardize(x_train)\n",
    "    x_test, _, _ = standardize(x_test, mean, std)\n",
    "\n",
    "    # fill nan\n",
    "    x_train = fill_nan(x_train)\n",
    "    x_test = fill_nan(x_test)\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 9,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_p, tX_test_p = preprocessing(tX, tX_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 226)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "source": [
    "feature_expansion(tx_0_filtered, 3).shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 12,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    test_indices = np.zeros(len(y)).astype(bool)\n",
    "    test_indices[k_indices[k]] = True\n",
    "    train_indices = (~test_indices).tolist()\n",
    "    test_indices = test_indices.tolist()\n",
    "        \n",
    "    x_train = x[train_indices, :]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    x_test = x[test_indices, :]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    \n",
    "    x_train_expanded = feature_expansion(x_train, degree)\n",
    "    x_test_expanded = feature_expansion(x_test, degree)\n",
    "\n",
    "\n",
    "    \n",
    "    w = ridge_regression(y_train, x_train_expanded, lambda_)\n",
    "    \n",
    "    \n",
    "    loss_tr = math.sqrt(2*compute_loss(y_train, x_train_expanded, w))\n",
    "    \n",
    "    loss_te = math.sqrt(2*compute_loss(y_test, x_test_expanded, w))\n",
    "    \n",
    "    classified = sum(predict_labels(w, x_test_expanded)==y_test)/len(y_test)\n",
    "    \n",
    "\n",
    "    return loss_tr, loss_te, classified"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_advanced_demo(x, y):\n",
    "    print(\"Dimensione x: \", x.shape)\n",
    "\n",
    "    seed = 1\n",
    "    degrees = np.arange(2,10)\n",
    "    k_fold = 1\n",
    "    #lambdas = np.logspace(-15, -8, 10)\n",
    "    lambdas = [1e-6]\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    \n",
    "\n",
    "    tr_total = []\n",
    "    te_total = []\n",
    "\n",
    "    min_te = float('inf')\n",
    "    min_degree = 0\n",
    "    min_lambda = 0\n",
    "    \n",
    "    for degree in degrees : \n",
    "        \n",
    "      #  print(\"vado con grado\", degree)\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        rmse_classification = []\n",
    "        rmse_tr_var = []\n",
    "        rmse_te_var = []\n",
    "    \n",
    "        for lambda_ in lambdas : \n",
    "        #    print(\"vado con lambda\", lambda_)\n",
    "\n",
    "\n",
    "            loss_tr = []\n",
    "            loss_te = []\n",
    "            loss_classification = []\n",
    "\n",
    "            for k in range(k_fold) : \n",
    "                tr, te, classified =  cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "                loss_tr.append(tr)\n",
    "                loss_te.append(te)\n",
    "                loss_classification.append(classified)\n",
    "\n",
    "            rmse_tr.append(np.mean(loss_tr))\n",
    "            rmse_classification.append(np.mean(loss_classification))\n",
    "\n",
    "\n",
    "            rmse_te.append(np.mean(loss_te))\n",
    "            rmse_tr_var.append(np.var(loss_tr))\n",
    "            rmse_te_var.append(np.var(loss_te))\n",
    "            \n",
    "            print(\"Grado\", degree, \", lambda\", lambda_, \", % giusti: \", np.mean(loss_classification))\n",
    "            \n",
    "            if np.mean(loss_te) < min_te : \n",
    "                min_te = np.mean(loss_te)\n",
    "                min_degree = degree\n",
    "                min_lambda = lambda_\n",
    "    \n",
    "        tr_total.append(rmse_tr)\n",
    "        te_total.append(rmse_te)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    cross_validation_advanced_visualization(lambdas, degrees, tr_total, te_total)\n",
    "    \n",
    "    print(\"min_degree\", min_degree)\n",
    "    print(\"min_lambda\", min_lambda)\n",
    "\n",
    "    \n",
    "    #cross_validation_advanced_visualization(lambdas, degrees, tr_total, te_total)\n",
    "    return lambdas, degrees, tr_total, te_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validation_advanced_demo(tX_p, y) \n"
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione x:  (250000, 29)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57/481305282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validation_advanced_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_57/230910994.py\u001b[0m in \u001b[0;36mcross_validation_advanced_demo\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassified\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mloss_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_57/4200033750.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML_project/ml-project-1-novae/scripts/implementations.py\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mlambda_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "cross_validation_advanced_demo(tX_p, y) \n"
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, degrees, tr_total, te_total = cross_validation_advanced_demo()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), tX_p]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\",True)\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "logistic_regression_penalized_gradient_descent_demo(y, tX_p)"
   ]
  },
  {
=======
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 10,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 31,
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> 439e7238e1a56e121a7902fcb3a60e07f8b2daf6
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
